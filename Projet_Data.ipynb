{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7142321,"sourceType":"datasetVersion","datasetId":4122440},{"sourceId":7142341,"sourceType":"datasetVersion","datasetId":4122452}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importation des modules necessaire a notre travail\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport os \nfrom PIL import Image \nfrom torch import nn,optim\nimport torch.nn.functional as F \nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A \nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nimport cv2\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-11T02:32:50.021381Z","iopub.execute_input":"2023-12-11T02:32:50.021743Z","iopub.status.idle":"2023-12-11T02:32:50.028712Z","shell.execute_reply.started":"2023-12-11T02:32:50.021718Z","shell.execute_reply":"2023-12-11T02:32:50.027762Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#chargement des données\ndf_train=pd.read_csv(\"/kaggle/input/dataset/Train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/dataset/Test.csv\")\nprint(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:32:58.765600Z","iopub.execute_input":"2023-12-11T02:32:58.766283Z","iopub.status.idle":"2023-12-11T02:32:58.791950Z","shell.execute_reply.started":"2023-12-11T02:32:58.766252Z","shell.execute_reply":"2023-12-11T02:32:58.791134Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"                   Image_id  Label\n0         id_004wknd7qd.jpg  blast\n1     id_004wknd7qd_rgn.jpg  blast\n2         id_005sitfgr2.jpg  brown\n3     id_005sitfgr2_rgn.jpg  brown\n4         id_00stp9t6m6.jpg  blast\n...                     ...    ...\n5335  id_zz6gzk7p97_rgn.jpg  brown\n5336      id_zz8ca2p67e.jpg  blast\n5337  id_zz8ca2p67e_rgn.jpg  blast\n5338      id_zzt8y9q0x0.jpg  brown\n5339  id_zzt8y9q0x0_rgn.jpg  brown\n\n[5340 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"#nombre d'occurrences de chaque classe et distribution normalisé en pourcentage\ndf_train['Label'].value_counts()\ndf_train['Label'].value_counts(normalize=True)*100","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:02.355943Z","iopub.execute_input":"2023-12-11T02:33:02.356739Z","iopub.status.idle":"2023-12-11T02:33:02.366261Z","shell.execute_reply.started":"2023-12-11T02:33:02.356704Z","shell.execute_reply":"2023-12-11T02:33:02.365344Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"Label\nblast      55.955056\nbrown      28.689139\nhealthy    15.355805\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"#suppression des doublons\ndf_train = df_train[~df_train.Image_id.str.contains('_rgn.jpg')].reset_index(drop=True)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:05.850624Z","iopub.execute_input":"2023-12-11T02:33:05.850990Z","iopub.status.idle":"2023-12-11T02:33:05.866534Z","shell.execute_reply.started":"2023-12-11T02:33:05.850963Z","shell.execute_reply":"2023-12-11T02:33:05.865507Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"            Image_id    Label\n0  id_004wknd7qd.jpg    blast\n1  id_005sitfgr2.jpg    brown\n2  id_00stp9t6m6.jpg    blast\n3  id_012zxewnhx.jpg    blast\n4  id_0186qwq2at.jpg  healthy","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_id</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_004wknd7qd.jpg</td>\n      <td>blast</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_005sitfgr2.jpg</td>\n      <td>brown</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_00stp9t6m6.jpg</td>\n      <td>blast</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_012zxewnhx.jpg</td>\n      <td>blast</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_0186qwq2at.jpg</td>\n      <td>healthy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Valeurs manquantes\ndf_train.isna().sum()\n#df_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:11.646498Z","iopub.execute_input":"2023-12-11T02:33:11.647227Z","iopub.status.idle":"2023-12-11T02:33:11.654801Z","shell.execute_reply.started":"2023-12-11T02:33:11.647193Z","shell.execute_reply":"2023-12-11T02:33:11.653859Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"Image_id    0\nLabel       0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#Encodage des labels\nlabel_encoder = LabelEncoder()\ndf_train['Classe'] = label_encoder.fit_transform(df_train['Label'])\ndf_train = df_train.drop('Label', axis=1)\n#blast 0 brown 1 healthy 2\nprint(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:14.415803Z","iopub.execute_input":"2023-12-11T02:33:14.416542Z","iopub.status.idle":"2023-12-11T02:33:14.426344Z","shell.execute_reply.started":"2023-12-11T02:33:14.416506Z","shell.execute_reply":"2023-12-11T02:33:14.425359Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"               Image_id  Classe\n0     id_004wknd7qd.jpg       0\n1     id_005sitfgr2.jpg       1\n2     id_00stp9t6m6.jpg       0\n3     id_012zxewnhx.jpg       0\n4     id_0186qwq2at.jpg       2\n...                 ...     ...\n2665  id_zydzdp046u.jpg       2\n2666  id_zyoowbqcm3.jpg       2\n2667  id_zz6gzk7p97.jpg       1\n2668  id_zz8ca2p67e.jpg       0\n2669  id_zzt8y9q0x0.jpg       1\n\n[2670 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preparation du modele","metadata":{}},{"cell_type":"code","source":"#division des données en apprentissage validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(df_train['Image_id'],df_train['Classe'], test_size=0.3, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:19.454667Z","iopub.execute_input":"2023-12-11T02:33:19.455354Z","iopub.status.idle":"2023-12-11T02:33:19.462677Z","shell.execute_reply.started":"2023-12-11T02:33:19.455318Z","shell.execute_reply":"2023-12-11T02:33:19.461798Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#transformations pour charger et prétraiter les images .\ntrain_transform = transforms.Compose([\n        transforms.RandomVerticalFlip(),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(degrees=(0,180)),\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n\n    ])\n\nval_transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:22.699094Z","iopub.execute_input":"2023-12-11T02:33:22.699479Z","iopub.status.idle":"2023-12-11T02:33:22.706360Z","shell.execute_reply.started":"2023-12-11T02:33:22.699449Z","shell.execute_reply":"2023-12-11T02:33:22.705421Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#Dataset et dataloaders\nclass TV_Dataset(Dataset):\n\n    def __init__(self, imag, labels, img_dir,transform):\n\n        self.labels = labels\n        self.img_dir = img_dir\n        self.imag = imag\n        self.transform = transform\n\n    def __len__(self):\n\n        return len(self.imag)\n\n    def __getitem__(self,idx):\n        img_path = os.path.join(self.img_dir, self.imag[idx])\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = Image.fromarray(img)\n        img = self.transform(img)\n        label = torch.tensor(self.labels[idx])\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:26.429111Z","iopub.execute_input":"2023-12-11T02:33:26.429793Z","iopub.status.idle":"2023-12-11T02:33:26.436600Z","shell.execute_reply.started":"2023-12-11T02:33:26.429756Z","shell.execute_reply":"2023-12-11T02:33:26.435662Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"#Creation d'instance train et validation\ntrain_dset = TV_Dataset(X_train.values, y_train.values, \"/kaggle/input/image1\",transform = train_transform)\nvalid_dset = TV_Dataset(X_val.values, y_val.values, \"/kaggle/input/image1\",transform = val_transform)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:35.957103Z","iopub.execute_input":"2023-12-11T02:33:35.957796Z","iopub.status.idle":"2023-12-11T02:33:35.962472Z","shell.execute_reply.started":"2023-12-11T02:33:35.957762Z","shell.execute_reply":"2023-12-11T02:33:35.961343Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dset, batch_size=150, shuffle=True)\nvalid_dataloader = DataLoader(valid_dset, batch_size = 150, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:42.037602Z","iopub.execute_input":"2023-12-11T02:33:42.037937Z","iopub.status.idle":"2023-12-11T02:33:42.042815Z","shell.execute_reply.started":"2023-12-11T02:33:42.037912Z","shell.execute_reply":"2023-12-11T02:33:42.041782Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"#Lot d'echantillon du dataloader et dimension\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"dimension : {train_features.size()}\")\nprint(f\"étiquette: {train_labels.size()}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:33:45.129192Z","iopub.execute_input":"2023-12-11T02:33:45.129596Z","iopub.status.idle":"2023-12-11T02:33:46.620742Z","shell.execute_reply.started":"2023-12-11T02:33:45.129562Z","shell.execute_reply":"2023-12-11T02:33:46.619800Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"dimension : torch.Size([150, 3, 224, 224])\nétiquette: torch.Size([150])\n","output_type":"stream"}]},{"cell_type":"code","source":"class Att(nn.Module):\n    def __init__(self, in_features, up_factor):\n        # Initialisation du module d'attention avec le nombre d'entités d'entrée (in_features)\n        # et le facteur d'échelle pour l'opération d'interpolation (up_factor)\n        super(Att, self).__init__()\n        self.up_factor = up_factor\n        self.op = nn.Conv2d(in_channels=in_features, out_channels=1, kernel_size=1, padding=0, bias=False)\n\n    def forward(self, l, g):\n        # Fonction forward, qui prend deux entrées, l (fonctionnalités locales) et g (fonctionnalités globales)\n        N, C, W, H = l.size()\n        \n        # Interpolation de g pour ajuster sa taille à celle de l\n        g = F.interpolate(g, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n        \n        # Combinaison des fonctionnalités locales et globales suivie d'une opération de convolution\n        c = self.op(l + g)  # batch_size x 1 x W x H\n\n        # Application de la fonction softmax pour obtenir les poids d'attention spatiaux\n        a = F.softmax(c.view(N, 1, -1), dim=2).view(N, 1, W, H)\n        \n        # Application du masque d'attention à l pour obtenir les fonctionnalités atténuées\n        g = torch.mul(a.expand_as(l), l)\n        g = g.view(N, C, -1).sum(dim=2)  # batch_size x C\n\n        # Retourne les poids d'attention spatiaux (a) et les fonctionnalités atténuées (g)\n        return a, g\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProjectBloc(nn.Module):\n    def __init__(self, in_feat, out_feat):\n\n        super(ProjectBloc, self).__init__()\n\n        self.op = nn.Conv2d(in_channels=in_feat, out_channels=out_feat, kernel_size=1, padding=0, bias=False)\n\n    def forward(self, inputs):\n\n        return self.op(inputs)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:34:01.157823Z","iopub.execute_input":"2023-12-11T02:34:01.158175Z","iopub.status.idle":"2023-12-11T02:34:01.163846Z","shell.execute_reply.started":"2023-12-11T02:34:01.158147Z","shell.execute_reply":"2023-12-11T02:34:01.162921Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nimport torch\n\nclass Att_ResNet34(nn.Module):\n\n    def __init__(self,classes):\n\n        super(Att_ResNet34, self).__init__()\n\n        base_model = models.resnet34(weights = \"IMAGENET1K_V1\")\n\n        layers = list(base_model.children())\n\n        self.conv1 = nn.Sequential(*layers[0:3])\n\n        self.conv2 = nn.Sequential(*layers[3:5])\n\n        self.conv3 = layers[5]\n\n        self.conv4 = layers[6]\n\n        self.conv5 = layers[7]\n\n        self.drp1 = nn.Dropout(0.3)\n        self.drp2 = nn.Dropout(0.4)\n\n        self.pool  = nn.AdaptiveAvgPool2d((1,1))\n\n        self.cls = nn.Linear(in_features = 512*3, out_features = classes, bias = True)\n\n        self.projector1 = ProjectBloc(128,512)\n        self.projector2 = ProjectBloc(256,512)\n\n        self.attn1 = Att(512,4)\n        self.attn2 = Att(512,2)\n\n    def forward(self,x):\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        l1 = self.conv3(x)\n        l2 = self.conv4(l1)\n        x = self.conv5(l2)\n\n        N, __, __, __ = x.size()\n        g = self.pool(x).view(N,512)\n\n        #print(x.size(),g.size(),l1.size(),l2.size())\n\n        x1,y1 = self.attn1(self.projector1(l1),x)\n        x2,y2 = self.attn2(self.projector2(l2),x)\n\n        g = torch.cat((g,y1,y2), dim = 1)\n\n        g = self.drp1(g)\n        g = self.drp2(g)\n\n        x = self.cls(g)\n\n        x = F.softmax(x,dim = 1)\n\n        return x,x1,x2\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:34:04.443706Z","iopub.execute_input":"2023-12-11T02:34:04.444058Z","iopub.status.idle":"2023-12-11T02:34:04.457334Z","shell.execute_reply.started":"2023-12-11T02:34:04.444031Z","shell.execute_reply":"2023-12-11T02:34:04.456365Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"MODEL","metadata":{}},{"cell_type":"code","source":"x = Att_ResNet34(3)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:34:13.479790Z","iopub.execute_input":"2023-12-11T02:34:13.480145Z","iopub.status.idle":"2023-12-11T02:34:13.866103Z","shell.execute_reply.started":"2023-12-11T02:34:13.480116Z","shell.execute_reply":"2023-12-11T02:34:13.865283Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\nclass_labels = np.unique(y_train.values)\nclass_weights = compute_class_weight(class_weight = 'balanced', classes = class_labels, y = y_train.values)\nclass_weights = torch.from_numpy(class_weights).float()\ncriterion = nn.CrossEntropyLoss(class_weights)\noptimizer = optim.Adam(x.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:34:20.659756Z","iopub.execute_input":"2023-12-11T02:34:20.660128Z","iopub.status.idle":"2023-12-11T02:34:20.669709Z","shell.execute_reply.started":"2023-12-11T02:34:20.660098Z","shell.execute_reply":"2023-12-11T02:34:20.668791Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Initialisation des listes pour stocker les pertes d'entraînement et de validation\ntrain_loss = []\nval_loss = []\n\n# Initialisation de la meilleure perte avec une valeur élevée pour s'assurer qu'elle sera mise à jour\nbest_loss = 1000\n\n# Boucle sur plusieurs époques (cycles d'entraînement)\nfor epoch in range(10):\n    \n    # Initialisation des pertes moyennes par époque\n    Average_train_loss_per_epoch = 0.0\n    Average_val_loss_per_epoch = 0.0\n\n    # Boucle sur les lots d'entraînement\n    for i, data in enumerate(train_dataloader, 0):\n        \n        # Initialisation de la perte par lot\n        running_train_loss_per_batch = 0.0\n\n        # Obtention des entrées et des étiquettes\n        inputs, labels = data\n\n        # Réinitialisation des gradients des paramètres\n        optimizer.zero_grad()\n\n        # Forward (passage avant), backward (rétropropagation) et optimisation\n        outputs, _, _ = x(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Mise à jour des statistiques d'entraînement\n        running_train_loss_per_batch += loss.item()\n        Average_train_loss_per_epoch += running_train_loss_per_batch\n\n        print(f'[{epoch + 1}, {i+1:3d}] loss: {running_train_loss_per_batch}')\n\n    print(\"\\n\")\n    print(f'epoch[{epoch + 1}] loss: {Average_train_loss_per_epoch / len(train_dataloader)}')\n    train_loss.append(Average_train_loss_per_epoch / len(train_dataloader))\n    print(\"\\n\")\n\n    # Validation du modèle sans mise à jour des gradients\n    with torch.no_grad():\n        for i, test_data in enumerate(valid_dataloader, 0):\n\n            # Initialisation de la perte de validation par lot\n            running_val_loss_per_batch = 0.0\n\n            # Obtention des entrées et des étiquettes de validation\n            val_inputs, val_labels = test_data\n\n            # Forward pass sur les données de validation\n            y_val, _, _ = x(val_inputs)\n\n            # Calcul de la perte de validation\n            loss = criterion(y_val, val_labels)\n\n            # Mise à jour des statistiques de perte de validation\n            running_val_loss_per_batch += loss.item()\n            Average_val_loss_per_epoch += running_val_loss_per_batch\n\n    # Calcul de la perte moyenne de validation par époque\n    val_loss_per_epoch = Average_val_loss_per_epoch / len(valid_dataloader)\n    print(f'epoch[{epoch + 1}] val_loss: {val_loss_per_epoch}')\n    val_loss.append(val_loss_per_epoch)\n\n    # Vérification si la perte de validation actuelle est meilleure que la meilleure perte précédente\n    if val_loss_per_epoch < best_loss:\n        torch.save(x.state_dict(), \"./poids.pth\")\n        best_loss = val_loss_per_epoch\n\nprint('Apprentissage Fini')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T02:34:53.197327Z","iopub.execute_input":"2023-12-11T02:34:53.197725Z","iopub.status.idle":"2023-12-11T03:33:54.902807Z","shell.execute_reply.started":"2023-12-11T02:34:53.197695Z","shell.execute_reply":"2023-12-11T03:33:54.901825Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"[1,   1] loss: 1.121924638748169\n[1,   2] loss: 1.0798803567886353\n[1,   3] loss: 1.0119836330413818\n[1,   4] loss: 0.8805481195449829\n[1,   5] loss: 0.9111342430114746\n[1,   6] loss: 0.8992314338684082\n[1,   7] loss: 0.8689140677452087\n[1,   8] loss: 0.9368747472763062\n[1,   9] loss: 0.900136411190033\n[1,  10] loss: 0.8703016042709351\n[1,  11] loss: 0.9333367943763733\n[1,  12] loss: 0.8880115747451782\n[1,  13] loss: 0.9497228860855103\n\n\nepoch[1] loss: 0.9424615777455844\n\n\nepoch[1] val_loss: 0.9360935986042023\n[2,   1] loss: 0.9252828359603882\n[2,   2] loss: 0.9745250940322876\n[2,   3] loss: 0.8623229265213013\n[2,   4] loss: 0.8296056389808655\n[2,   5] loss: 0.9768058657646179\n[2,   6] loss: 0.8939516544342041\n[2,   7] loss: 0.9757348895072937\n[2,   8] loss: 0.8848392963409424\n[2,   9] loss: 0.8771483302116394\n[2,  10] loss: 0.8017147183418274\n[2,  11] loss: 0.8363415598869324\n[2,  12] loss: 0.8940817713737488\n[2,  13] loss: 0.807083010673523\n\n\nepoch[2] loss: 0.8876490455407363\n\n\nepoch[2] val_loss: 0.8830363353093466\n[3,   1] loss: 0.7849810719490051\n[3,   2] loss: 0.8299616575241089\n[3,   3] loss: 0.8685797452926636\n[3,   4] loss: 0.836261510848999\n[3,   5] loss: 0.901236355304718\n[3,   6] loss: 0.859610378742218\n[3,   7] loss: 0.9221535325050354\n[3,   8] loss: 0.8079798221588135\n[3,   9] loss: 0.8184147477149963\n[3,  10] loss: 0.8063346743583679\n[3,  11] loss: 0.8282471299171448\n[3,  12] loss: 0.8037421107292175\n[3,  13] loss: 0.7739197015762329\n\n\nepoch[3] loss: 0.8339555722016555\n\n\nepoch[3] val_loss: 0.889714260896047\n[4,   1] loss: 0.8291512131690979\n[4,   2] loss: 0.828292727470398\n[4,   3] loss: 0.8232794404029846\n[4,   4] loss: 0.7530272006988525\n[4,   5] loss: 0.8509970903396606\n[4,   6] loss: 0.8249187469482422\n[4,   7] loss: 0.8798155784606934\n[4,   8] loss: 0.7510637044906616\n[4,   9] loss: 0.7572571635246277\n[4,  10] loss: 0.8324386477470398\n[4,  11] loss: 0.8263756632804871\n[4,  12] loss: 0.8403290510177612\n[4,  13] loss: 0.8143426775932312\n\n\nepoch[4] loss: 0.8162529927033645\n\n\nepoch[4] val_loss: 0.8652243514855703\n[5,   1] loss: 0.7884203195571899\n[5,   2] loss: 0.8007692694664001\n[5,   3] loss: 0.7376901507377625\n[5,   4] loss: 0.860156774520874\n[5,   5] loss: 0.7632102370262146\n[5,   6] loss: 0.8334103226661682\n[5,   7] loss: 0.767305850982666\n[5,   8] loss: 0.7914938926696777\n[5,   9] loss: 0.8587939143180847\n[5,  10] loss: 0.8818090558052063\n[5,  11] loss: 0.8432174921035767\n[5,  12] loss: 0.8250700831413269\n[5,  13] loss: 0.8069888353347778\n\n\nepoch[5] loss: 0.8121797075638404\n\n\nepoch[5] val_loss: 0.8494015137354533\n[6,   1] loss: 0.7490124702453613\n[6,   2] loss: 0.8110383749008179\n[6,   3] loss: 0.8314067125320435\n[6,   4] loss: 0.8885785341262817\n[6,   5] loss: 0.776094377040863\n[6,   6] loss: 0.7702258825302124\n[6,   7] loss: 0.7852398157119751\n[6,   8] loss: 0.7687320709228516\n[6,   9] loss: 0.8220222592353821\n[6,  10] loss: 0.7310898900032043\n[6,  11] loss: 0.7818964123725891\n[6,  12] loss: 0.8319545388221741\n[6,  13] loss: 0.8617284893989563\n\n\nepoch[6] loss: 0.8006938329109778\n\n\nepoch[6] val_loss: 0.8080963591734568\n[7,   1] loss: 0.7475938200950623\n[7,   2] loss: 0.7834246158599854\n[7,   3] loss: 0.8517352938652039\n[7,   4] loss: 0.7803235054016113\n[7,   5] loss: 0.7485054731369019\n[7,   6] loss: 0.7040102481842041\n[7,   7] loss: 0.7577819228172302\n[7,   8] loss: 0.7996457815170288\n[7,   9] loss: 0.8245614767074585\n[7,  10] loss: 0.7469931244850159\n[7,  11] loss: 0.8273728489875793\n[7,  12] loss: 0.7641727328300476\n[7,  13] loss: 0.7700957655906677\n\n\nepoch[7] loss: 0.7774012776521536\n\n\nepoch[7] val_loss: 0.8028336465358734\n[8,   1] loss: 0.7616550922393799\n[8,   2] loss: 0.754685640335083\n[8,   3] loss: 0.7465707063674927\n[8,   4] loss: 0.7259914875030518\n[8,   5] loss: 0.7668128609657288\n[8,   6] loss: 0.779958963394165\n[8,   7] loss: 0.8350752592086792\n[8,   8] loss: 0.7954916954040527\n[8,   9] loss: 0.7543395757675171\n[8,  10] loss: 0.7344821095466614\n[8,  11] loss: 0.7033826112747192\n[8,  12] loss: 0.847878634929657\n[8,  13] loss: 0.7355144619941711\n\n\nepoch[8] loss: 0.7647568537638738\n\n\nepoch[8] val_loss: 0.8234663307666779\n[9,   1] loss: 0.8131088614463806\n[9,   2] loss: 0.8679028153419495\n[9,   3] loss: 0.789793074131012\n[9,   4] loss: 0.7472559809684753\n[9,   5] loss: 0.7750611305236816\n[9,   6] loss: 0.8641493916511536\n[9,   7] loss: 0.7999719977378845\n[9,   8] loss: 0.816373884677887\n[9,   9] loss: 0.8743467926979065\n[9,  10] loss: 0.8083856105804443\n[9,  11] loss: 0.7916980981826782\n[9,  12] loss: 0.7679935693740845\n[9,  13] loss: 0.8946533203125\n\n\nepoch[9] loss: 0.816207271355849\n\n\nepoch[9] val_loss: 0.8395543495814005\n[10,   1] loss: 0.806731104850769\n[10,   2] loss: 0.7405869364738464\n[10,   3] loss: 0.7079975008964539\n[10,   4] loss: 0.7842968702316284\n[10,   5] loss: 0.8240812420845032\n[10,   6] loss: 0.8009827733039856\n[10,   7] loss: 0.7821685671806335\n[10,   8] loss: 0.8019528388977051\n[10,   9] loss: 0.8140990138053894\n[10,  10] loss: 0.7233691215515137\n[10,  11] loss: 0.7897109985351562\n[10,  12] loss: 0.8605802059173584\n[10,  13] loss: 0.789472222328186\n\n\nepoch[10] loss: 0.7866176458505484\n\n\nepoch[10] val_loss: 0.7828872899214426\nApprentissage Fini\n","output_type":"stream"}]},{"cell_type":"code","source":"test_transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:34:38.635310Z","iopub.execute_input":"2023-12-11T03:34:38.635784Z","iopub.status.idle":"2023-12-11T03:34:38.641311Z","shell.execute_reply.started":"2023-12-11T03:34:38.635749Z","shell.execute_reply":"2023-12-11T03:34:38.640332Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"class Dataset_Test(Dataset):\n\n    def __init__(self, images, img_rep,transform):\n\n        self.img_rep = img_rep\n        self.images = images\n        self.transform = transform\n\n    def __len__(self):\n\n        return len(self.images)\n\n    def __getitem__(self,idx):\n\n        img_path = os.path.join(self.img_rep, self.images[idx])\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = Image.fromarray(img)\n        img = self.transform(img)\n        image_name = self.images[idx]\n\n        return img,image_name\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:36:37.559429Z","iopub.execute_input":"2023-12-11T03:36:37.559814Z","iopub.status.idle":"2023-12-11T03:36:37.566795Z","shell.execute_reply.started":"2023-12-11T03:36:37.559784Z","shell.execute_reply":"2023-12-11T03:36:37.565822Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"df_test = df_test[~df_test.Image_id.str.contains('_rgn.jpg')].reset_index(drop=True)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:36:44.631037Z","iopub.execute_input":"2023-12-11T03:36:44.631378Z","iopub.status.idle":"2023-12-11T03:36:44.642249Z","shell.execute_reply.started":"2023-12-11T03:36:44.631353Z","shell.execute_reply":"2023-12-11T03:36:44.641246Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"            Image_id\n0  id_00vl5wvxq3.jpg\n1  id_01hu05mtch.jpg\n2  id_030ln10ewn.jpg\n3  id_03z57m8xht.jpg\n4  id_04ngep1w4b.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_00vl5wvxq3.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_01hu05mtch.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_030ln10ewn.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_03z57m8xht.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_04ngep1w4b.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = Dataset_Test(df_test['Image_id'].values,\"/kaggle/input/image1\",transform = test_transform )","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:36:51.342287Z","iopub.execute_input":"2023-12-11T03:36:51.342687Z","iopub.status.idle":"2023-12-11T03:36:51.347246Z","shell.execute_reply.started":"2023-12-11T03:36:51.342657Z","shell.execute_reply":"2023-12-11T03:36:51.346344Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"test_dataloader = DataLoader(test_dataset, batch_size = 150, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:36:53.182995Z","iopub.execute_input":"2023-12-11T03:36:53.183910Z","iopub.status.idle":"2023-12-11T03:36:53.188514Z","shell.execute_reply.started":"2023-12-11T03:36:53.183874Z","shell.execute_reply":"2023-12-11T03:36:53.187537Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"x = Att_ResNet34(3)\nx.load_state_dict(torch.load(\"/kaggle/working/poids.pth\"))\nx.eval()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:36:56.555437Z","iopub.execute_input":"2023-12-11T03:36:56.555772Z","iopub.status.idle":"2023-12-11T03:36:57.018003Z","shell.execute_reply.started":"2023-12-11T03:36:56.555748Z","shell.execute_reply":"2023-12-11T03:36:57.017324Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"Att_ResNet34(\n  (conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv2): Sequential(\n    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (conv3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (conv4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (conv5): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (drp1): Dropout(p=0.3, inplace=False)\n  (drp2): Dropout(p=0.4, inplace=False)\n  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (cls): Linear(in_features=1536, out_features=3, bias=True)\n  (projector1): ProjectBloc(\n    (op): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  )\n  (projector2): ProjectBloc(\n    (op): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  )\n  (attn1): Att(\n    (op): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  )\n  (attn2): Att(\n    (op): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndf = {'Image_id': [], 'blast_prob': [], 'brown_prob': [], 'healthy_prob': []}\n\nwith torch.no_grad():\n    for data in test_dataloader:\n        images, names = data\n        outputs, _, _ = x(images)\n        \n        proba = F.softmax(outputs, dim=1)\n\n        for i in range(len(proba)):\n            df['Image_id'].append(names[i])\n            df['blast_prob'].append(proba[i][0].item())\n            df['brown_prob'].append(proba[i][1].item())\n            df['healthy_prob'].append(proba[i][2].item())\n\nresultat = pd.DataFrame(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:37:20.795207Z","iopub.execute_input":"2023-12-11T03:37:20.795971Z","iopub.status.idle":"2023-12-11T03:38:32.761210Z","shell.execute_reply.started":"2023-12-11T03:37:20.795935Z","shell.execute_reply":"2023-12-11T03:38:32.760210Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"resultat","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:38:41.479659Z","iopub.execute_input":"2023-12-11T03:38:41.480020Z","iopub.status.idle":"2023-12-11T03:38:41.495475Z","shell.execute_reply.started":"2023-12-11T03:38:41.479986Z","shell.execute_reply":"2023-12-11T03:38:41.494345Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"               Image_id  blast_prob  brown_prob  healthy_prob\n0     id_00vl5wvxq3.jpg    0.576117    0.211942      0.211942\n1     id_01hu05mtch.jpg    0.373918    0.393209      0.232874\n2     id_030ln10ewn.jpg    0.341428    0.305732      0.352840\n3     id_03z57m8xht.jpg    0.576117    0.211942      0.211942\n4     id_04ngep1w4b.jpg    0.576117    0.211942      0.211942\n...                 ...         ...         ...           ...\n1140  id_zrdlgjrq3r.jpg    0.211951    0.211976      0.576074\n1141  id_zsfayxwipp.jpg    0.212985    0.215909      0.571107\n1142  id_ztvp2l9k3h.jpg    0.254790    0.521993      0.223218\n1143  id_zwwcma7hlt.jpg    0.576117    0.211942      0.211942\n1144  id_zyo7m4fj8h.jpg    0.576117    0.211942      0.211942\n\n[1145 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_id</th>\n      <th>blast_prob</th>\n      <th>brown_prob</th>\n      <th>healthy_prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_00vl5wvxq3.jpg</td>\n      <td>0.576117</td>\n      <td>0.211942</td>\n      <td>0.211942</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_01hu05mtch.jpg</td>\n      <td>0.373918</td>\n      <td>0.393209</td>\n      <td>0.232874</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_030ln10ewn.jpg</td>\n      <td>0.341428</td>\n      <td>0.305732</td>\n      <td>0.352840</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_03z57m8xht.jpg</td>\n      <td>0.576117</td>\n      <td>0.211942</td>\n      <td>0.211942</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_04ngep1w4b.jpg</td>\n      <td>0.576117</td>\n      <td>0.211942</td>\n      <td>0.211942</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>id_zrdlgjrq3r.jpg</td>\n      <td>0.211951</td>\n      <td>0.211976</td>\n      <td>0.576074</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>id_zsfayxwipp.jpg</td>\n      <td>0.212985</td>\n      <td>0.215909</td>\n      <td>0.571107</td>\n    </tr>\n    <tr>\n      <th>1142</th>\n      <td>id_ztvp2l9k3h.jpg</td>\n      <td>0.254790</td>\n      <td>0.521993</td>\n      <td>0.223218</td>\n    </tr>\n    <tr>\n      <th>1143</th>\n      <td>id_zwwcma7hlt.jpg</td>\n      <td>0.576117</td>\n      <td>0.211942</td>\n      <td>0.211942</td>\n    </tr>\n    <tr>\n      <th>1144</th>\n      <td>id_zyo7m4fj8h.jpg</td>\n      <td>0.576117</td>\n      <td>0.211942</td>\n      <td>0.211942</td>\n    </tr>\n  </tbody>\n</table>\n<p>1145 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Obtenez le chemin du répertoire de travail sur Kaggle\nkaggle_output_dir = '/kaggle/working/'\n\n# Spécifiez le nom du fichier CSV\noutput_csv_filename = 'submission_results2.csv'\n\n# Construisez le chemin complet du fichier CSV\noutput_csv_path = os.path.join(kaggle_output_dir, output_csv_filename)\n\n# Enregistrez le DataFrame dans le fichier CSV\nresultat.to_csv(output_csv_path, index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:47:00.072893Z","iopub.execute_input":"2023-12-11T03:47:00.073777Z","iopub.status.idle":"2023-12-11T03:47:00.090098Z","shell.execute_reply.started":"2023-12-11T03:47:00.073742Z","shell.execute_reply":"2023-12-11T03:47:00.089257Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}